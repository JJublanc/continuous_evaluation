{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian A/B testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_name = \"bayesian_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not image_file_name in os.listdir():\n",
    "    os.mkdir(\"bayesian_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The use-case__\n",
    "\n",
    "One wants to compare the performances between two actions. For instance one wants to know if it is worth displaying an advertisment at the begining of a video. To answer this question we will display the advertisment to a random group of customers (group A let's say) and to the other customers we will display nothing. Then we will get the conversion rate in each group and statistically test if the group to which the advertisment has been diplayed convert more or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bayesian approach of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources : \n",
    "* https://medium.com/convoy-tech/the-power-of-bayesian-a-b-testing-f859d2219d5\n",
    "* https://cdn2.hubspot.net/hubfs/310840/VWO_SmartStats_technical_whitepaper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach use the bayesian rules to update, all along the test, the assumption about the distribution of law probability about the conversion rate for each group.\n",
    "\n",
    "One will then have distributions of conversion rate's probabilities for each group that will get sharper along the test. These distributions will then be used to decide if one can stop the test and choose the better option with a good confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's use Beta distributions. At the beginning we will use $(1,1)$ as parameters so that we have a uniform distribution. Then the more we will get data, de more sharpe will be our distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_values = [(1, 1), (3, 3), (11, 11), (31, 31)]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for a, b in params_values:\n",
    "        prior_beta_law = scs.beta(a, b)\n",
    "        x = np.linspace(-0.25, 1.25, 100)\n",
    "        plt.plot(x, prior_beta_law.pdf(x))\n",
    "\n",
    "plt.title(\"Beta law density function\")\n",
    "plt.legend([\"(a,b) = {}\".format(x) for x in params_values])\n",
    "plt.xlabel(\"True conversion rate\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.savefig(\"{}/density_beta_groupe2.png\".format(image_file_name, a, b))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to perform the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true conversion rate in groupe A is called $\\lambda_A$, the one the group B is $\\lambda_B$\n",
    "\n",
    "We will make a fake example by choosing a true conversion rate of $\\lambda_A = 0.5$ for group A and a true conversion rate of $\\lambda_B = 0.4$ for group B.\n",
    "\n",
    "As mentionned before, the prior for each group is a Beta law with parameters $(1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_a = 0.5\n",
    "lambda_b = 0.4\n",
    "\n",
    "prior_alpha_a = 1\n",
    "prior_beta_a = 1\n",
    "\n",
    "prior_alpha_b = 1\n",
    "prior_beta_b = 1\n",
    "\n",
    "steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample 10 000 points for both of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_a = np.random.binomial(1, lambda_a, 10000)\n",
    "results_b = np.random.binomial(1, lambda_b, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the posterior laws thanks to the information given by the results. The posterior density functions are called respectivelly $f_A$ and $f_B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior_laws(results_a, results_b,\n",
    "                           prior_alpha_a=1, prior_beta_a=1, \n",
    "                           prior_alpha_b=1, prior_beta_b=1):\n",
    "    \n",
    "    posterior_alpha_a = prior_alpha_a + np.sum(results_a)\n",
    "    posterior_beta_a = prior_beta_a + len(results_a) - np.sum(results_a)\n",
    "\n",
    "    posterior_alpha_b = prior_alpha_b + np.sum(results_b)\n",
    "    posterior_beta_b = prior_beta_b + len(results_b) - np.sum(results_b)\n",
    "    \n",
    "    posterior_law_a = scs.beta(posterior_alpha_a, posterior_beta_a)\n",
    "    posterior_law_b = scs.beta(posterior_alpha_b, posterior_beta_b)\n",
    "    \n",
    "    return posterior_law_a, posterior_law_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_law_a, posterior_law_b = compute_posterior_laws(results_a, results_b,\n",
    "                                                          prior_alpha_a, prior_beta_a, \n",
    "                                                          prior_alpha_b, prior_beta_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the law we can compute the probability that the conversion rate belong to a small intervalle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior_probs(posterior_law_a, posterior_law_b, steps=100):\n",
    "    \n",
    "    posterior_a = []\n",
    "    posterior_b = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        inf_ = i * (1 / (steps))\n",
    "        sup_ = (i + 1) * (1 / (steps))\n",
    "        posterior_a.append(posterior_law_a.cdf(sup_) - posterior_law_a.cdf(inf_))\n",
    "        posterior_b.append(posterior_law_b.cdf(sup_) - posterior_law_b.cdf(inf_))\n",
    "    \n",
    "    return posterior_a, posterior_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_a, posterior_b = compute_posterior_probs(posterior_law_a, posterior_law_b, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the posterior information, we will joint them to and get a joint density function $f_{A,B}$ such as :\n",
    "$$ \\forall(x_A, x_B) \\in [0,1]^2, f_{A,B}(x_A, x_B) = f_A(x_A).f_B(x_B)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_posterior_probs(posterior_a, posterior_b, steps=100):\n",
    "    \n",
    "    posterior_joint = np.zeros((steps, steps))\n",
    "    \n",
    "    for i in range(steps):\n",
    "        for j in range(steps):\n",
    "            posterior_joint[i, j] = posterior_a[i] * posterior_b[j]\n",
    "            \n",
    "    return posterior_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_joint_probs = compute_joint_posterior_probs(posterior_a, posterior_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute $P(\\lambda_A > \\lambda_B)$ and $P(\\lambda_B < \\lambda_A)$ given by :\n",
    "$$P(\\lambda_B > \\lambda_A)   = \\displaystyle\\int_0^1\\int_{x_A}^{1} f_{A,B}(x_A, x_B)d x_B dx_A$$\n",
    "\n",
    "and \n",
    "\n",
    "$$P(\\lambda_A > \\lambda_B)   = \\displaystyle\\int_0^1\\int_{x_B}^{1} f_{A,B}(x_A, x_B)d x_A dx_B$$\n",
    "\n",
    "We can be use to estimate the probability of being when concidering a result for the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a great error should have a greater weight that a small one, because the consequencies are worse. This is why we use a loss function taking into account the difference between $\\lambda_A$ and $\\lambda_B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we find $\\lambda_A >\\lambda_B$ we will compute the following loss :\n",
    "$$E\\big[\\max(\\lambda_B - \\lambda_A, 0) | n_A, n_B, c_A, c_B\\big]   = \\displaystyle\\int_0^1\\int_{x_A}^{1} (x_B-x_A).f_{A,B}(x_A, x_B)d x_B dx_A$$\n",
    "\n",
    "and conversly we will compute that one if we find the opposite :\n",
    "$$E\\big[\\max(\\lambda_A - \\lambda_B, 0) | n_A, n_B, c_A, c_B\\big]   = \\displaystyle\\int_0^1\\int_{x_B}^{1} (x_A-x_B).f_{A,B}(x_A, x_B)d x_A dx_B$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_probability(posterior_joint_probs, steps):\n",
    "        \n",
    "    proba_b_sup_a = 0\n",
    "    for i in range(steps):\n",
    "        for j in range(i + 1, steps):\n",
    "            proba_b_sup_a += posterior_joint_probs[i, j]\n",
    "            \n",
    "    proba_a_sup_b = 0\n",
    "    for i in range(steps):\n",
    "        for j in range(0, i):\n",
    "            proba_a_sup_b += posterior_joint_probs[i, j]\n",
    "            \n",
    "    proba_a_equal_b = 0\n",
    "    for i in range(steps):\n",
    "        proba_a_equal_b += posterior_joint_probs[i, i]\n",
    "    \n",
    "    return proba_b_sup_a, proba_a_sup_b, proba_a_equal_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_error_probability(posterior_joint_probs, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : centrer la loss\n",
    "# var=\"A\" signifie que l'on se met dans l'hypthèse où lambda_a >= lambda_b\n",
    "\n",
    "def punctual_loss(i, j, var, steps):\n",
    "    if var == \"A\":\n",
    "        return max(j * (1 / steps) - i * (1/steps), 0)\n",
    "\n",
    "    if var == \"B\":\n",
    "        return max(i * (1 / steps) - j * (1/steps), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(posterior_joint_probs, var, steps):\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(steps):\n",
    "        for j in range(steps):\n",
    "            loss += posterior_joint_probs[i, j] * punctual_loss(i, j, var, steps)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnally we run the test untill one of the losses associated with each group is less than a threshold $\\epsilon$ chosen thanks to business conceiderations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_baeysian_test(results_a, results_b, steps=100, epsilon=0.001):\n",
    "    stop_test = False\n",
    "    test = None\n",
    "    \n",
    "    posterior_law_a, posterior_law_b = compute_posterior_laws(results_a, results_b)\n",
    "    posterior_a, posterior_b = compute_posterior_probs(posterior_law_a, posterior_law_b, steps)\n",
    "    posterior_joint_probs = compute_joint_posterior_probs(posterior_a, posterior_b, steps)\n",
    "\n",
    "    # results\n",
    "    proba_b_sup_a, proba_a_sup_b, proba_a_equal_b = compute_error_probability(posterior_joint_probs, steps)\n",
    "    \n",
    "    loss_a = compute_loss(posterior_joint_probs, \"A\", steps)\n",
    "    loss_b = compute_loss(posterior_joint_probs, \"B\", steps)\n",
    "    \n",
    "    groupe_a_loss_inf_epsilon = loss_a < epsilon\n",
    "    groupe_b_loss_inf_epsilon = compute_loss(posterior_joint_probs, \"B\", steps) < epsilon\n",
    "\n",
    "    print('loss A inf to epsilon : {}'.format(groupe_a_loss_inf_epsilon))\n",
    "    print('loss B inf to epsilon : {}\\n'.format(groupe_b_loss_inf_epsilon))\n",
    "\n",
    "    print(\"mean group A : {}\".format(np.mean(results_a)))\n",
    "    print(\"mean group B : {}\\n\".format(np.mean(results_b)))\n",
    "\n",
    "    print(\"proba mean B sup to mean A : {}\".format(proba_b_sup_a))\n",
    "    print(\"proba mean A sup to mean B : {}\\n\".format(proba_a_sup_b))\n",
    "\n",
    "    if (np.mean(results_a) > np.mean(results_b)) & groupe_a_loss_inf_epsilon:\n",
    "        stop_test = True\n",
    "        test = \"A\"\n",
    "        print(\"_________________________________________\")\n",
    "        print(\"group A have a significant highest mean\")\n",
    "        print(\"_________________________________________\")\n",
    "\n",
    "    elif (np.mean(results_b) > np.mean(results_a)) & groupe_b_loss_inf_epsilon:\n",
    "        stop_test = True\n",
    "        test = \"B\"\n",
    "        print(\"_________________________________________\")\n",
    "        print(\"group B have a significant highest mean\")\n",
    "        print(\"_________________________________________\")\n",
    "\n",
    "    return test, stop_test, loss_a, loss_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we make several plots to show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_joint_density(results_a, results_b, number, steps=100, save=False):\n",
    "\n",
    "    posterior_law_a, posterior_law_b = compute_posterior_laws(results_a, results_b)\n",
    "    posterior_a, posterior_b = compute_posterior_probs(posterior_law_a, posterior_law_b, steps)\n",
    "    posterior_joint_probs = compute_joint_posterior_probs(posterior_a, posterior_b)\n",
    "\n",
    "    # plot heatmap of the joint probs\n",
    "    plt.imshow(posterior_joint_probs, cmap=\"YlGn\"\n",
    "                                       #'hot'\n",
    "               , interpolation='nearest')\n",
    "\n",
    "    # flip the y axis \n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    # plot the identity line\n",
    "    plt.plot((0, 99), (0, 99), color='#00664d', linewidth=2)\n",
    "\n",
    "    # set the axis' labels\n",
    "    plt.xlabel(\"True mean group B\", fontsize = 12)\n",
    "    plt.ylabel(\"True mean group A\", fontsize = 12)\n",
    "\n",
    "    # set the ticks' labels\n",
    "    plt.xticks([x*10 for x in range(10)], [round(10 * i * (1 / steps), 2) for i in range(10)], rotation='vertical')\n",
    "    plt.yticks([x*10 for x in range(10)], [round(10 * i * (1 / steps), 2) for i in range(10)], rotation='horizontal')\n",
    "\n",
    "    # title\n",
    "    plt.title(\"Density\\n\"\\\n",
    "              \"Size each group : {}\".format(len(results_a), len(results_b)))\n",
    "    \n",
    "    if save :\n",
    "        plt.savefig(\"{}/density_test_{}_steps.png\".format(image_file_name, number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in range(1, 100):\n",
    "    plot_joint_density(results_a[:t], results_b[:t], t, True)\n",
    "\n",
    "for t in range(1, 100):\n",
    "    i = t * 100\n",
    "    plot_joint_density(results_a[:i], results_b[:i], i, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 100\n",
    "group_with_highest_mean, stop_test, loss_a, loss_b = proceed_baeysian_test(results_a[:t], results_b[:t], t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 30):\n",
    "    group_with_highest_mean, stop_test = proceed_baeysian_test(results_a[:i*100], results_b[:i*100], steps=100, epsilon=0.01)\n",
    "    if stop_test:\n",
    "        break\n",
    "print(i*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = 300\n",
    "\n",
    "res_a = results_a[:t]\n",
    "res_b = results_b[:t]\n",
    "\n",
    "\n",
    "plot_joint_density(res_a, res_b, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
